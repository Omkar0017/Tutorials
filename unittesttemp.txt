1. Read Data from Oracle and Store in HDFS as a File. 

DataConnect has an Oracle Data Source that can be used to read the data from PROD oracle DB in a scheduled manner and send it to the feed. There a plugin called save to file in DataConnect that can save the processed data multiple file format. Using this plugin data can be saved in the form of CSV in our desired file directory. Once all data is moved this csv can be used to create a Hive Table. 

2. Create a Hive External Table on Top of the File in HDFS 

Once data is available in CSV format an External Hive Table can be created on top of that. Hive will treat this data as a structured table you can query with SQL. Example of creating a Hive table on top of CSV is 

CREATE EXTERNAL TABLE employees ( 

  id INT, 

  name STRING, 

  department STRING, 

  salary DOUBLE 

) 

ROW FORMAT DELIMITED 

FIELDS TERMINATED BY ',' 

STORED AS TEXTFILE 

LOCATION '/user/hadoop/oracle_exports/'; 

 

3. Query the Data in Hive using HUE 

Once the Table is created data can be fetched like SQL using HUE. Hue has inbuild SQL engine and editor that accepts SQL like query and returns result. 

 

Summary of the Flow 

1.Oracle DB →Read using DataConnect  

2.Using DataConnect save-to-File plugin → save data in CSV Format. 

3.Hive/Impala external table → Create Table on top of the CSV file. 

4.Structured SQL querying → Using Hue query through the Data. 

 
